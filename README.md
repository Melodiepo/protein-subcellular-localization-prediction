# Benchmarking Transformer-Based Embeddings for Protein Subcellular Localization
### *A Comparative Study of LSTMs and Protein Language Models*

---

## **Abstract**

In this study, we present a systematic comparison of computational models for protein subcellular localization (PSL) prediction across four compartments—cytosolic, secreted, mitochondrial, and nuclear—using a dataset of 7,458 training sequences and 20 test sequences. Our modeling framework spans classical machine learning (SVM), LSTM-based deep learning architectures, and transformer-based embedding methods with MLP classifiers. Results demonstrate that embeddings generated by protein language models—particularly **ESM2-T33**—achieve **83.4% macro-F1** and **81.9% accuracy** on the validation set, underscoring the power of large pretrained embeddings for PSL.

---

## **Repository Overview**

## **1. Data Loading**

- **`data_fasta.py`**  
  - Parses raw FASTA files (e.g., `\_parse_fasta` and `\_get_data`) into dictionaries of sequences, lengths, and labels.  

- **`pickle_data.py`**  
  - Converts the dictionaries into pickled training/test sets, using a unified `get_data` interface.  
  - Ensures directories exist, automatically generating pickle files if missing.

- **`config.py`**  
  - Centralizes constants such as paths, class names, and maximum sequence length.

- **`preprocessing.py`**  
  - **`build_dictionary`**: Creates a token-to-ID mapping, pads/truncates sequences.  
  - **`summary_stats`**: Generates descriptive tables of sequence length distributions.

---

## **2. Feature Extraction**

- **`feature_extraction.py`**  
  - Cleans sequences and computes:
    - **Amino acid composition** (global + local).
    - **Molecular properties** (pI, weight).
    - **Signal motifs** (regex-based).
    - **Physicochemical properties** (GRAVY, aromaticity, etc.).
  - Bundles all into a **71D** feature vector for each sequence.

---

## **3. Exploratory Data Analysis (EDA)**

- Uses **`summary_stats`** and basic plots (histograms, correlation heatmaps, boxplots, etc.) to explore:
  - Sequence-length distributions.
  - Class‐wise statistics (e.g., isoelectric point, motif frequencies).
  - Feature correlations to identify potential redundancies.

---

## **4. LSTM Model Fitting**

- **`experiments.py`**  
  - Contains `experiment_*` functions (e.g., `experiment_lstm_no_attention`, `experiment_cnn_lstm_no_attention`, etc.) with a shared `_run_experiment` pattern for data shuffling, k-fold splits, and logging.

- **`trainer.py`**  
  - Implements a `run` class orchestrating the training loop (mini-batches, forward pass, backprop, and validation).  
  - Generates confusion matrices and can track attention weights if the model includes an attention mechanism.

- **`model_definitions.py`**  
  - Houses model architectures like `attentionLSTM` or `CNN_lstm`, optionally implementing attention layers.

---

## **5. Transformer-Based Embeddings Fitting**

- **`embedding.py`**  
  - **`compute_and_save_embeddings`**: Runs a pretrained transformer (e.g., ESM2-T33) once per sequence, saving hidden states to `.pt` files.

- **`ProteinDataset`** (within `embedding.py` or `preprocessing.py`)  
  - Tokenizes raw sequences for transformers (ProtBERT, XLNet, etc.).

- **`PrecomputedEmbeddingDataset`**  
  - Wraps the `.pt` embeddings in a `Dataset` for quick loading into PyTorch DataLoaders.

- **`train_classifier`**  
  - Trains an MLP on loaded embeddings, recording final confusion matrices, losses, and accuracies.

---

## **6. Model Evaluation**

1. **Result Aggregation**  
   - The `.pkl` files in `metrics/` store training/validation histories, confusion matrices, etc., named by model type (e.g. `esm_results.pkl`).

2. **`extract_metrics`**  
   - Normalizes result tuples (4 vs. 8 elements) for uniform plotting of training/validation curves.

3. **Per-Class and Overall Metrics**  
   - Derives precision, recall, F1 (macro-avg + class-avg) and ROC-AUC, culminating in final comparison tables or heatmaps.

---

## **7. Cross-Validation, Retraining, & Blind Test**

- **5-Fold CV**  
  - `experiment_esm_mlp_kfold` (or similar) is used for cross-validation on the full training set, saving the mean confusion matrix across folds.

- **Test Embeddings**  
  - Blind test sequences are embedded with the same transformer pipeline, generating a `.pt` file without labels.

- **Retraining & Prediction**  
  - A function (e.g. `train_on_full_data_and_test_esm`) trains on all available training embeddings, then infers the unlabeled test set.  
  - Outputs a DataFrame with predicted classes, probabilities, and confidence levels (High/Medium/Low).

---

## **8. Feature Analysis**

- **Integrated Gradients** (via `captum.attr.IntegratedGradients`):  
  - Identifies dimension-wise attributions for ESM2 embeddings, optionally computed per-class.

- **Saliency & Permutation Importance**  
  - **Saliency** focuses on gradient magnitude, while **Permutation** importance measures drops in accuracy upon shuffling each embedding dimension.

- **t-SNE**  
  - Projects either the full 1280D embeddings or top “important” dimensions into 2D for cluster visualization.

- **Attention Analysis**  
  - If using an attention-based LSTM, scripts like `attention_viz.py` can generate attention heatmaps for specific classes.

---

## **Dependencies**

All required Python packages are listed in **`requirements.txt`**.
